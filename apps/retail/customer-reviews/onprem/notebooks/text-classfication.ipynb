{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "This Notebook focuses on implementing multi-class text classification on Amazon automotive reviews dataset by choosing any one combination of various data transformation techniques and algorithms.\n",
    "\n",
    "Rating(1-5) is predicted for each review from the dataset.\n",
    "\n",
    "Best scoring combinations are listed below. Any single combination out of the following can be chosen for data transformation & model training :\n",
    "\n",
    "* Creation of word embeddings using gensim's word2vec & subsequent training using Random Forest algorithm.\n",
    "* Creation of word embeddings using word2vec and/or Smooth Inverse Frequency (SIF) technique & subsequent training using Random Forest algorithm.\n",
    "* Vectorisation using Term frequency-inverse document frequency (Tfidf) technique & subsequent training using Random Forest algorithm.\n",
    "* Vectorisation using Tfidf technique & subsequent training using Linear support vector clustering (SVC) algorithm.\n",
    "\n",
    "| Data Transformation  | Training Algorithm |\n",
    "| ------------- | ------------- |\n",
    "| Word2vec | Random Forest  |\n",
    "| Word2vec + SIF  | Random Forest  |\n",
    "| TfIdf Vectorization  | Random Forest  |\n",
    "| TfIdf Vectorization  | Linear SVC  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cisco-kubeflow-starter-pack'...\n",
      "remote: Enumerating objects: 359, done.\u001b[K\n",
      "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
      "remote: Compressing objects: 100% (244/244), done.\u001b[K\n",
      "remote: Total 4927 (delta 116), reused 292 (delta 72), pack-reused 4568\u001b[K\n",
      "Receiving objects: 100% (4927/4927), 23.60 MiB | 40.97 MiB/s, done.\n",
      "Resolving deltas: 100% (1845/1845), done.\n"
     ]
    }
   ],
   "source": [
    "BRANCH_NAME=\"dev\" #Provide git branch name \"master\" or \"dev\"\n",
    "! git clone -b $BRANCH_NAME https://github.com/CiscoAI/cisco-kubeflow-starter-pack.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 19.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 115.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gensim\n",
      "  Downloading gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 30.9 MB/s eta 0:00:01    |███████████████▌                | 11.8 MB 30.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn==0.20.3\n",
      "  Downloading scikit_learn-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 13.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imbalanced-learn==0.4.3\n",
      "  Downloading imbalanced_learn-0.4.3-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 79.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 210 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "\u001b[K     |████████████████████████████████| 300 kB 121.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2020.7.14-cp36-cp36m-manylinux2010_x86_64.whl (660 kB)\n",
      "\u001b[K     |████████████████████████████████| 660 kB 118.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 1.3 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/lib/python3/dist-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.1.0.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 94.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Collecting boto\n",
      "  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 84.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.14.48.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.6)\n",
      "Collecting botocore<1.18.0,>=1.17.48\n",
      "  Downloading botocore-1.17.48-py2.py3-none-any.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 99.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 20.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk, sklearn, smart-open, boto3\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1435508 sha256=e6ae2fe116a722937ace200df1fba7bb1fb27af99c06dd86bf54982641c7bff2\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=2397 sha256=32a17a17901e153de9c207bf9b373e89d1ac75ed9a9a8c540c96a10e73f39453\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=117753 sha256=71935ebb71607cd9af3543e44a4874cb459b44adb2738ab5964aad4b6fe74b68\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a4/9b/d5/85705a7ab783cd6f7bd718f01d3b1396272f30044e3c36401a\n",
      "  Building wheel for boto3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for boto3: filename=boto3-1.14.48-py2.py3-none-any.whl size=126740 sha256=590667c6364f904a7a566fb09675b9480f71eb9d2cf6c260ffe01f7a8bc1bef7\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/65/c9/6c/a3bb693a4684274b841014e4a4afff0d1a110849f15b6680e9\n",
      "Successfully built nltk sklearn smart-open boto3\n",
      "Installing collected packages: pandas, click, joblib, regex, tqdm, nltk, boto, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim, scikit-learn, sklearn, imbalanced-learn\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed boto-2.49.0 boto3-1.14.48 botocore-1.17.48 click-7.1.2 docutils-0.15.2 gensim-3.8.3 imbalanced-learn-0.4.3 jmespath-0.10.0 joblib-0.16.0 nltk-3.5 pandas-1.1.1 regex-2020.7.14 s3transfer-0.3.3 scikit-learn-0.20.3 sklearn-0.0 smart-open-2.1.0 tqdm-4.48.2\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas nltk gensim sklearn scikit-learn==0.20.3 imbalanced-learn==0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Jupyter notebook kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import yaml\n",
    "from joblib import dump\n",
    "import re\n",
    "import nltk as nl\n",
    "import gensim\n",
    "import yaml\n",
    "import os\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#nltk\n",
    "from nltk.corpus import stopwords\n",
    "nl.download('punkt')\n",
    "nl.download('stopwords')\n",
    "\n",
    "#Over-sampling\n",
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataset from JSON format to CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"cisco-kubeflow-starter-pack/apps/retail/customer-reviews/onprem\"\n",
    "json_data = pd.read_json(os.path.join(path, \"data/amazon_automotive_reviews.json\"), lines=True)\n",
    "json_data.to_csv('amazon_automotive_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('amazon_automotive_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3F73SC1LY51OO</td>\n",
       "      <td>B00002243X</td>\n",
       "      <td>Alan Montgomery</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>I needed a set of jumper cables for my new car...</td>\n",
       "      <td>5</td>\n",
       "      <td>Work Well - Should Have Bought Longer Ones</td>\n",
       "      <td>1313539200</td>\n",
       "      <td>08 17, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A20S66SKYXULG2</td>\n",
       "      <td>B00002243X</td>\n",
       "      <td>alphonse</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>These long cables work fine for my truck, but ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Okay long cables</td>\n",
       "      <td>1315094400</td>\n",
       "      <td>09 4, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2I8LFSN2IS5EO</td>\n",
       "      <td>B00002243X</td>\n",
       "      <td>Chris</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Can't comment much on these since they have no...</td>\n",
       "      <td>5</td>\n",
       "      <td>Looks and feels heavy Duty</td>\n",
       "      <td>1374710400</td>\n",
       "      <td>07 25, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3GT2EWQSO45ZG</td>\n",
       "      <td>B00002243X</td>\n",
       "      <td>DeusEx</td>\n",
       "      <td>[19, 19]</td>\n",
       "      <td>I absolutley love Amazon!!!  For the price of ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent choice for Jumper Cables!!!</td>\n",
       "      <td>1292889600</td>\n",
       "      <td>12 21, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3ESWJPAVRPWB4</td>\n",
       "      <td>B00002243X</td>\n",
       "      <td>E. Hernandez</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I purchased the 12' feet long cable set and th...</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent, High Quality Starter Cables</td>\n",
       "      <td>1341360000</td>\n",
       "      <td>07 4, 2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin     reviewerName   helpful  \\\n",
       "0  A3F73SC1LY51OO  B00002243X  Alan Montgomery    [4, 4]   \n",
       "1  A20S66SKYXULG2  B00002243X         alphonse    [1, 1]   \n",
       "2  A2I8LFSN2IS5EO  B00002243X            Chris    [0, 0]   \n",
       "3  A3GT2EWQSO45ZG  B00002243X           DeusEx  [19, 19]   \n",
       "4  A3ESWJPAVRPWB4  B00002243X     E. Hernandez    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  I needed a set of jumper cables for my new car...        5   \n",
       "1  These long cables work fine for my truck, but ...        4   \n",
       "2  Can't comment much on these since they have no...        5   \n",
       "3  I absolutley love Amazon!!!  For the price of ...        5   \n",
       "4  I purchased the 12' feet long cable set and th...        5   \n",
       "\n",
       "                                      summary  unixReviewTime   reviewTime  \n",
       "0  Work Well - Should Have Bought Longer Ones      1313539200  08 17, 2011  \n",
       "1                            Okay long cables      1315094400   09 4, 2011  \n",
       "2                  Looks and feels heavy Duty      1374710400  07 25, 2013  \n",
       "3       Excellent choice for Jumper Cables!!!      1292889600  12 21, 2010  \n",
       "4      Excellent, High Quality Starter Cables      1341360000   07 4, 2012  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['overallRating'] = raw_data['overall']\n",
    "raw_data = raw_data.drop(['reviewerID','asin','reviewerName','helpful','overall','summary','unixReviewTime','reviewTime'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean review text column and remove punctuations and numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['p_review'] = raw_data['reviewText'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','', str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose data transformation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation can be done using either word2vec or Smooth inverse frequency (sif) technique or Tf-Idf vectorization (tfidf).\n",
    "# Choose from ==> ['word2vec', 'sif', 'tfidf']\n",
    "\n",
    "data_transform = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose model training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training can be done using either random forest (rf) or Linear Support vector clustering (lsvc) algorithms.\n",
    "#Choose from ==> ['rf','lsvc']\n",
    "\n",
    "train_algorithm = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate data transformation method & training algorithm options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data_transform or data_transform not in ['word2vec', 'sif', 'tfidf']:\n",
    "     raise ValueError(\"Set a valid method to perform data transformation (word2vec/sif/tfidf)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_algorithm or train_algorithm not in ['rf','lsvc']:\n",
    "     raise ValueError(\"Set a valid algorithm to train your model(rf/lsvc)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_transform in ['word2vec','sif'] and train_algorithm == 'lsvc':\n",
    "    raise Warning(\"The combination selected may not be the best scoring one!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply data transformation on data as per selected choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_transform in ['word2vec', 'sif']:\n",
    "\n",
    "        p_review = raw_data['p_review'].to_list()\n",
    "\n",
    "        tokens = [nl.word_tokenize(sentences) for sentences in p_review]\n",
    "\n",
    "        stop_words = stopwords.words('english')\n",
    "\n",
    "        tokens = [[word for word in tokens[i] if not word in stopwords.words('english')] for i in range(len(tokens))]\n",
    "\n",
    "        wv_model = gensim.models.Word2Vec(tokens, size=300, min_count=1, workers=4)\n",
    "\n",
    "        wv_model.train(tokens, total_examples=len(tokens), epochs=50)\n",
    "        \n",
    "        print(\"Word2vec model generated & trained on tokens from review text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data using TfIdf vectorization..\n",
      "(20473, 23968)\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "if data_transform == 'word2vec':\n",
    "        \n",
    "        print(\"Preparing training data using word2vec..\")\n",
    "        wv_train = []\n",
    "        for i in range(len(tokens)):\n",
    "            wv_train.append(np.mean(np.asarray([wv_model[token] for token in tokens[i]]),axis=0))\n",
    "        print(\"Completed\")\n",
    "            \n",
    "elif data_transform == 'sif':\n",
    "    \n",
    "        print(\"Preparing training data using Smooth inverse frequency(SIF)..\")\n",
    "        vlookup = wv_model.wv.vocab\n",
    "        Z = 0\n",
    "        for k in vlookup:\n",
    "                Z += vlookup[k].count # Compute the normalization constant Z\n",
    "\n",
    "        a = 0.001\n",
    "        embedding_size = 300\n",
    "        wv_sif_train = []\n",
    "        for i in range(len(tokens)):\n",
    "                vs = np.zeros(300)\n",
    "                for word in tokens[i]:\n",
    "                        a_value = a / (a + (vlookup[word].count/Z))\n",
    "                        vs = np.add(vs, np.multiply(a_value, wv_model.wv[word]))\n",
    "                wv_sif_train.append(np.divide(vs, len(tokens[i])))\n",
    "        print(\"Completed\")\n",
    "                \n",
    "elif data_transform == 'tfidf':\n",
    "         print(\"Preparing training data using TfIdf vectorization..\")\n",
    "         tfidf = TfidfVectorizer(ngram_range=(1,2),sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', stop_words='english')\n",
    "         features = tfidf.fit_transform(raw_data.p_review).toarray()\n",
    "         print(features.shape)\n",
    "         print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depict class imbalance issue in dataset using value count for each rating\n",
    "\n",
    "Here rating 5 has lot more records than others, so the dataset is considered to be highly skewed / imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    13928\n",
       "4     3967\n",
       "3     1430\n",
       "2      606\n",
       "1      542\n",
       "Name: overallRating, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.overallRating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize target variable to a local variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = raw_data.overallRating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample dataset to remove class imbalance issue using SMOTENC\n",
    "\n",
    "Preprocessing of the dataset is done in such a way that the rating categories other than 5 ( which is the majority class) is oversampled accordingly, so as to get a balanced dataset without any prediction output bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset samples per class Counter({5: 13928, 4: 11000, 3: 6800, 2: 6200, 1: 6000})\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTENC(sampling_strategy={1: 6000, 2: 6200, 3 : 6800, 4: 11000}, random_state=42, categorical_features=[1])\n",
    "if data_transform == 'word2vec':\n",
    "    X_resampled, y_resampled = sm.fit_resample(np.asarray(wv_train), y)\n",
    "    \n",
    "elif data_transform == 'sif':\n",
    "    X_resampled, y_resampled = sm.fit_resample(np.asarray(wv_sif_train), y)\n",
    "    \n",
    "elif data_transform == 'tfidf':\n",
    "    X_resampled, y_resampled = sm.fit_resample(features, y)\n",
    "\n",
    "print('Resampled dataset samples per class {}'.format(Counter(y_resampled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_transform == 'word2vec':\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_resampled,y_resampled,test_size=0.3,shuffle=True,random_state=7)\n",
    "\n",
    "elif data_transform == 'sif':\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_resampled,y_resampled,test_size=0.3,shuffle=True,random_state=7)\n",
    "\n",
    "elif data_transform == 'tfidf':\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_resampled,y_resampled,test_size=0.3,shuffle=True,random_state=7)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_algorithm == 'rf':\n",
    "    model = RandomForestClassifier(n_estimators=40, random_state=0)\n",
    "    model.fit(x_train,y_train)\n",
    "    \n",
    "elif train_algorithm == 'lsvc':\n",
    "    model = LinearSVC()\n",
    "    model.fit(x_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/model/model.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/home/jovyan'\n",
    "file_rel_path = 'model/'\n",
    "file_abs_path = os.path.join(file_path, file_rel_path)\n",
    "file_name = 'model.joblib'\n",
    "\n",
    "if not os.path.exists(file_rel_path):\n",
    "    os.mkdir(file_rel_path)\n",
    "dump(model, file_abs_path + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define inference service name & model storage URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvc://workspace-test/model/\n"
     ]
    }
   ],
   "source": [
    "svc_name = 'text-classify'\n",
    "\n",
    "!kubectl get pods $HOSTNAME -o yaml -n anonymous > podspec\n",
    "with open(\"podspec\") as f:\n",
    "    content = yaml.safe_load(f)\n",
    "    for elm in content['spec']['volumes']:\n",
    "        if 'workspace-' in elm['name']:\n",
    "            pvc = elm['name']\n",
    "os.remove('podspec')\n",
    "pvc\n",
    "    \n",
    "storageURI = \"pvc://\" + pvc + '/' + file_rel_path\n",
    "print(storageURI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define configuration for inference service creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: serving.kubeflow.org/v1alpha2\r\n",
      "kind: InferenceService\r\n",
      "metadata:\r\n",
      "  name: text-classify\r\n",
      "  namespace: anonymous\r\n",
      "spec:\r\n",
      "  default:\r\n",
      "    predictor:\r\n",
      "      sklearn:\r\n",
      "        storageUri: pvc://workspace-test/model/\r\n"
     ]
    }
   ],
   "source": [
    "wsvol_blerssi_kf = f\"\"\"apiVersion: \"serving.kubeflow.org/v1alpha2\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: {svc_name}\n",
    "  namespace: anonymous\n",
    "spec:\n",
    "  default:\n",
    "    predictor:\n",
    "      sklearn:\n",
    "        storageUri: {storageURI}\n",
    "\"\"\"\n",
    "    \n",
    "kfserving = yaml.safe_load(wsvol_blerssi_kf)\n",
    "with open('blerssi-kfserving.yaml', 'w') as file:\n",
    "    yaml_kfserving = yaml.dump(kfserving,file)\n",
    "\n",
    "! cat blerssi-kfserving.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the configuration .yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kubeflow.org/text-classify created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f blerssi-kfserving.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether inferenceservice is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            URL                                                                  READY   DEFAULT TRAFFIC   CANARY TRAFFIC   AGE\r\n",
      "text-classify   http://text-classify.anonymous.example.com/v1/models/text-classify   True    100                                2m41s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get inferenceservice -n anonymous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "Wait for inference service READY=\"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict data from serving after setting INGRESS_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions\n",
      "[4, 3, 2, 1, 5, 5, 1, 1, 4, 5, 2, 2, 5, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "host_name = svc_name + '.anonymous.example.com'\n",
    "\n",
    "headers = { \n",
    "    'host': host_name\n",
    "}\n",
    "\n",
    "for i in range(15):\n",
    "\n",
    "    formData = {\n",
    "        'instances': x_test[i:i+1].tolist()\n",
    "    }\n",
    "    url = 'http://<<INGRESS IP>>:<<INGRESS PORT>>/v1/models/' + svc_name + ':predict'\n",
    "    res = requests.post(url, json=formData, headers=headers)\n",
    "    results = res.json()\n",
    "    prediction = results['predictions']\n",
    "    predictions.append(prediction[0])\n",
    "    \n",
    "print(\"Predictions\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up after predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete inference service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kubeflow.org \"text-classify\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f blerssi-kfserving.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete model folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $file_rel_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
